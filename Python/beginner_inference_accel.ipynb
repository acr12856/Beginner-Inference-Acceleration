{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install dependencies\n",
        "!pip install -q torch torchvision onnx onnxruntime-gpu onnxruntime-tools onnxconverter-common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sId883ZXlz-c",
        "outputId": "e332f265-017d-4702-963b-b9e2dea4017d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import onnx\n",
        "import os\n",
        "import time\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "from onnxconverter_common import float16\n",
        "import onnxruntime as ort"
      ],
      "metadata": {
        "id": "JstSdfYkmWFG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export pretrained ResNet18 to ONNX\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "model.eval()\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"resnet-18.onnx\",\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    opset_version=11,\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
        ")\n",
        "\n",
        "print(\"Exported resnet-18.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFPJ2klnmecW",
        "outputId": "4b5d1bf0-ceee-4e97-9c71-7ebbb069800a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 186MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported resnet-18.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model (currently FP32)\n",
        "model = onnx.load(\"resnet-18.onnx\")\n",
        "\n",
        "# Convert it to FP16 for lower latency during inference\n",
        "model_fp16 = float16.convert_float_to_float16(model)\n",
        "\n",
        "# Create ONNX file for our quantized model\n",
        "onnx.save(model_fp16, \"resnet-18_quantized.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuSuU7sNmfL3",
        "outputId": "ccbb29b2-f85a-4a29-d3ff-7b4ab9caacc7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:52: UserWarning: the float32 number 2.3695349682384403e-08 will be truncated to 1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -4.769529660109129e-09 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:52: UserWarning: the float32 number 8.528004475317141e-17 will be truncated to 1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -1.037211034503102e-16 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -1.6570577798802333e-08 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:52: UserWarning: the float32 number 1.0116913817109774e-12 will be truncated to 1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -1.4509890684799576e-12 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -4.215191751200109e-08 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -6.735274382663192e-08 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:52: UserWarning: the float32 number 1.4782583157568752e-08 will be truncated to 1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -3.421217797949794e-08 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -3.4513492064291995e-09 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:52: UserWarning: the float32 number 3.9312606503472125e-08 will be truncated to 1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -8.7634655088209e-09 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:52: UserWarning: the float32 number 5.640451306021532e-08 will be truncated to 1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -6.639092475779762e-08 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:52: UserWarning: the float32 number 8.207661750248008e-08 will be truncated to 1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:52: UserWarning: the float32 number 3.3710478852810866e-09 will be truncated to 1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -1.561287987783544e-08 will be truncated to -1e-07\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:70: UserWarning: the float32 number -1.989902997934223e-08 will be truncated to -1e-07\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmark function\n",
        "def benchmark(model_path, runs=100, provider=\"CUDAExecutionProvider\"):\n",
        "    print(f\"Benchmarking {model_path}\")\n",
        "\n",
        "    sess = ort.InferenceSession(model_path, providers=[provider])\n",
        "    input_name = sess.get_inputs()[0].name\n",
        "    dummy = np.random.randn(1, 3, 224, 224)\n",
        "\n",
        "    # Convert dummy input to float16 if the model is quantized\n",
        "    if \"quantized\" in model_path:\n",
        "        dummy = dummy.astype(np.float16)\n",
        "    else:\n",
        "        dummy = dummy.astype(np.float32)\n",
        "\n",
        "\n",
        "    # Initial runs to get overhead/optimizations out of the way\n",
        "    for _ in range(15):\n",
        "        sess.run(None, {input_name: dummy})\n",
        "\n",
        "    # Actual benchmarking\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(runs):\n",
        "        sess.run(None, {input_name: dummy})\n",
        "    end = time.perf_counter()\n",
        "\n",
        "    avg_ms = (end - start) / runs * 1000\n",
        "    print(f\"{model_path} avg latency: {avg_ms:.2f} ms\\n\")"
      ],
      "metadata": {
        "id": "WrSyubqnmro1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run benchmarks on GPU\n",
        "benchmark(\"resnet-18.onnx\")\n",
        "benchmark(\"resnet-18_quantized.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_NN0XjsmtPv",
        "outputId": "09647fd1-227d-4a0d-febc-681cf825f863"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking resnet-18.onnx\n",
            "resnet-18.onnx avg latency: 3.70 ms\n",
            "\n",
            "Benchmarking resnet-18_quantized.onnx\n",
            "resnet-18_quantized.onnx avg latency: 1.90 ms\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
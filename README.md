# ONNX ResNet-18 Inference Benchmark

A performance comparison of ResNet-18 inference using ONNX Runtime, featuring both Python and C++ implementations, comparing standard FP32 inference with FP16 quantized inference.

## Python Implementation (Jupyter Notebook) via Google Colab

The notebook is designed to [run on Google Colab](https://colab.research.google.com/drive/1hU-r7YtrpH6ZnzORXi92d0L4ZOlzbMWt?usp=sharing) with T4 GPU support. 

### What does this notebook do?
A pretrained ResNet-18 is exported to ONNX format, and is then converted from a FP32 model to a FP16 model to display simple quantization. Both models are benchmarked, with warm-up runs to eliminate any overhead. 

### Performance Results (T4 GPU)
- **FP32 Model**: ~3.70 ms average latency
- **FP16 Model**: ~1.90 ms average latency (~49% improvement)

## C++ Implementation

### Prerequisites
- CMake 3.12+
- ONNX Runtime C++ library
- C++17 compatible compiler

### Setup Instructions

1. **Install ONNX Runtime**

   Download the appropriate ONNX Runtime package for your platform from the [official releases](https://github.com/microsoft/onnxruntime/releases).

2. **Configure Paths**

   Edit the root `CMakeLists.txt` file and set the following variables to match your ONNX Runtime installation:
   ```cmake
   set(ORT_LIB_PATH "/path/to/onnxruntime/lib")
   set(ORT_INCLUDE_PATH "/path/to/onnxruntime/include")
   ```

3. **Build the Project**

   ```bash
   cd C++
   cmake -B build
   cmake --build build
   ```

4. **Run the Benchmark**

   ```bash
   cd build/src
   ./main
   ```

### What does the C++ version do?
Both FP32 and FP16 ONNX models are loaded, and 1000 inference iterations are run per model (with 10 warmup iterations to eliminate behind-the-scenes optimizations). We use the CPU execution provider, but this is configurable. 

## Model Generation

The ONNX models are generated by the Python implementation:

1. **resnet-18.onnx**: Original FP32 model exported from PyTorch
2. **resnet-18_quantized.onnx**: FP16 quantized version for improved performance

Both models accept input tensors of shape `[1, 3, 224, 224]` (batch_size=1, RGB channels, 224x224 image).

## Performance Considerations

### GPU vs CPU
The Python implementation uses GPU execution (`CUDAExecutionProvider`), whereas the C++ implementation uses CPU execution by default. GPU execution typically provides better performance for larger models.

### Quantization Benefits
FP16 quantization reduces the model size by ~50%, which results in latency improvements during inference (varies depending on hardware). In exchange, a slight accuracy drop can be expected. 

## Customization

### Changing Execution Providers
**Python**: Modify the `provider` parameter in the `benchmark()` function:
```python
benchmark("resnet-18.onnx", provider="CPUExecutionProvider")
```

**C++**: Update the session options and memory info creation in `main.cpp`:
```cpp
// Enable GPU inference
Ort::SessionOptions session_options;
session_options.AppendExecutionProvider_CUDA(0);  // GPU ID 0
...
// Use GPU memory for input/output tensors
Ort::MemoryInfo mem_info = Ort::MemoryInfo::CreateCuda(OrtDeviceAllocator, OrtMemTypeDefault);
```
NOTE: You will need to have the appropriate ONNX library downloaded for your hardware. You can check which providers you have with:
```cpp
auto providers = Ort::GetAvailableProviders();
for (const auto& p : providers) {
    std::cout << "Available provider: " << p << std::endl;
}
```

### Adjusting Benchmark Parameters
- **Python**: Change `runs` parameter in benchmark function calls
- **C++**: Modify `num_runs` parameter in `run_inference()` calls
